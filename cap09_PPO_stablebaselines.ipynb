{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/pablo-sampaio/cinbalada/blob/main/cap09_PPO_stablebaselines.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hyyN-2qyK_T2"
      },
      "source": [
        "# PPO com Stable Baselines3\n",
        "\n",
        "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/pablo-sampaio/rl_facil/blob/main/cap09/cap09-PPO-stablebaselines.ipynb) \n",
        "\n",
        "Vamos usar o algoritmo **PPO** (*Proximal Policy Optimization*) neste Google Colab.\n",
        "\n",
        "Referências:\n",
        "- Stable-Baselines3: https://github.com/DLR-RM/stable-baselines3\n",
        "- Documentação: https://stable-baselines.readthedocs.io/en/master/\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xVm9QPNVwKXN"
      },
      "source": [
        "## 1 - Configurações necessárias"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "thRA4CT9kH4G"
      },
      "source": [
        "### 1.1 Instalação de pacotes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "gWskDE2c9WoN"
      },
      "outputs": [],
      "source": [
        "from IPython.display import clear_output\n",
        "!apt-get install ffmpeg freeglut3-dev xvfb  # For visualization\n",
        "!pip install gym==00.25.1\n",
        "!pip install gym[box2d]\n",
        "!pip install gym[atari,accept-rom-license]\n",
        "!pip install stable-baselines3[extra]\n",
        "\n",
        "clear_output()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pLlWktrRj9GZ"
      },
      "source": [
        "### 1.2 Para salvar vídeo"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "MPyfQxD5z26J"
      },
      "outputs": [],
      "source": [
        "# Set up fake display; otherwise rendering will fail\n",
        "import os\n",
        "os.system(\"Xvfb :1 -screen 0 1024x768x24 &\")\n",
        "os.environ['DISPLAY'] = ':1'"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LTRNUfulOGaF"
      },
      "source": [
        "A gravação é feita com o wrapper [VecVideoRecorder](https://stable-baselines.readthedocs.io/en/master/guide/vec_envs.html#vecvideorecorder)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "Trag9dQpOIhx"
      },
      "outputs": [],
      "source": [
        "from stable_baselines3.common.vec_env import VecVideoRecorder, DummyVecEnv\n",
        "\n",
        "def record_video(env_id, model, video_length=500, prefix='', video_folder='videos/'):\n",
        "  \"\"\"\n",
        "  :param env_id: (str)\n",
        "  :param model: (RL model)\n",
        "  :param video_length: (int)\n",
        "  :param prefix: (str)\n",
        "  :param video_folder: (str)\n",
        "  \"\"\"\n",
        "  eval_env = DummyVecEnv([lambda: gym.make(env_id)])\n",
        "  # Start the video at step=0 and record the given number of steps\n",
        "  eval_env = VecVideoRecorder(eval_env, video_folder=video_folder,\n",
        "                              record_video_trigger=lambda step: step == 0, video_length=video_length,\n",
        "                              name_prefix=prefix)\n",
        "\n",
        "  obs = eval_env.reset()\n",
        "  for _ in range(video_length):\n",
        "    action, _ = model.predict(obs)\n",
        "    obs, _, _, _ = eval_env.step(action)\n",
        "\n",
        "  # Close the video recorder\n",
        "  eval_env.close()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "SLzXxO8VMD6N"
      },
      "outputs": [],
      "source": [
        "import base64\n",
        "from pathlib import Path\n",
        "\n",
        "from IPython import display as ipythondisplay\n",
        "\n",
        "def show_videos(video_path='', prefix=''):\n",
        "  \"\"\"\n",
        "  Taken from https://github.com/eleurent/highway-env\n",
        "\n",
        "  :param video_path: (str) Path to the folder containing videos\n",
        "  :param prefix: (str) Filter the video, showing only the only starting with this prefix\n",
        "  \"\"\"\n",
        "  html = []\n",
        "  for mp4 in Path(video_path).glob(\"{}*.mp4\".format(prefix)):\n",
        "      video_b64 = base64.b64encode(mp4.read_bytes())\n",
        "      html.append('''<video alt=\"{}\" autoplay \n",
        "                    loop controls style=\"height: 400px;\">\n",
        "                    <source src=\"data:video/mp4;base64,{}\" type=\"video/mp4\" />\n",
        "                </video>'''.format(mp4, video_b64.decode('ascii')))\n",
        "  ipythondisplay.display(ipythondisplay.HTML(data=\"<br>\".join(html)))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FtY8FhliLsGm"
      },
      "source": [
        "### 1.3 Imports"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "BIedd7Pz9sOs",
        "outputId": "0ce0f2f4-1048-4abd-a2c1-a89643796112"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'1.6.0'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 5
        }
      ],
      "source": [
        "import gym\n",
        "import numpy as np\n",
        "import tensorboard\n",
        "\n",
        "%load_ext tensorboard\n",
        "\n",
        "import stable_baselines3\n",
        "stable_baselines3.__version__"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JmKML-NNXibO"
      },
      "source": [
        "Abaixo, importamos a classe que representa o algoritmo **PPO** e diversas funções auxiliares."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "wgAXxClR0BfH"
      },
      "outputs": [],
      "source": [
        "from stable_baselines3 import PPO\n",
        "from stable_baselines3.common.evaluation import evaluate_policy\n",
        "\n",
        "# para treinar com ambientes Atari\n",
        "from stable_baselines3.common.env_util import make_atari_env\n",
        "from stable_baselines3.common.vec_env import VecFrameStack"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-0_8OQbOTTNT"
      },
      "source": [
        "Abaixo, importamos a classe que vai representar a **rede neural da política**. No caso, importamos uma rede com camadas totalmente conectadas (MLP) e uma rede convolucional (CNN), com arquiteturas (camadas, etc) pré-definidas. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "ROUJr675TT01"
      },
      "outputs": [],
      "source": [
        "from stable_baselines3.ppo import MlpPolicy, CnnPolicy"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RapkYvTXL7Cd"
      },
      "source": [
        "## 2 - Cria o ambiente e instancia o agente\n",
        "\n",
        "Estamos criando o ambiente e passando para a classe **PPO**, que permite testar ou treinar o agente.\n",
        "\n",
        "Abaixo, criamos o PPO com os valores default dos seus parâmetros mais importantes. Definimos apenas estes parâmetros:\n",
        "- `tensorboard_log`, que indica o diretório dos logs \n",
        "- `verbose=0` para evitar mostrar muitas mensagens.\n",
        "- mais informações: https://stable-baselines3.readthedocs.io/en/master/modules/ppo.html \n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pUWGZp3i9wyf",
        "outputId": "9435c4d8-d1ca-4629-f0be-8ca514395ae0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Using cpu device\n",
            "Wrapping the env with a `Monitor` wrapper\n",
            "Wrapping the env in a DummyVecEnv.\n"
          ]
        }
      ],
      "source": [
        "env = gym.make('LunarLander-v2')\n",
        "\n",
        "model = PPO(MlpPolicy, env, tensorboard_log=\"log_dir\", verbose=0)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rAM4wBawmNex"
      },
      "source": [
        "Para rodar com **jogos de Atari**, descomente o código abaixo.\n",
        "\n",
        "Obs.: Talvez você não consiga usar a função para gerar os vídeos."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8Wdw9LzwaFeN",
        "outputId": "0eb18a43-d29b-4fdb-df3d-6efb98339555"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using cpu device\n",
            "Wrapping the env in a VecTransposeImage.\n"
          ]
        }
      ],
      "source": [
        "# Esta função cria o ambiente, junto com wrappers especializados\n",
        "#env = make_atari_env('PongNoFrameskip-v4', n_envs=4, seed=0)\n",
        "\n",
        "# Wrapper para fazer cada estado ser uma \"pilha\" de 4 frames\n",
        "#env = VecFrameStack(env, n_stack=4)\n",
        "\n",
        "# Cria com uma rede convolucional como política\n",
        "#model = PPO(CnnPolicy, env, tensorboard_log=\"log_dir\", verbose=1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zjEVOIY8NVeK"
      },
      "source": [
        "Vamos avaliar o agente não-treinado:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xDHLMA6NFk95",
        "outputId": "5d1a1e96-14dc-4550-9fb6-b8f9e7216147"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Retorno médio: -17.53 +/- 2.42\n"
          ]
        }
      ],
      "source": [
        "# Random Agent, before training\n",
        "mean_reward, std_reward = evaluate_policy(model, env, n_eval_episodes=30)\n",
        "\n",
        "print(f\"Retorno médio: {mean_reward:.2f} +/- {std_reward:.2f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BTbAq8Ji_EBf"
      },
      "source": [
        "E agora vamos gravar um vídeo para vê-lo em ação, antes de treinado."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OKo8i2aw_Iz5"
      },
      "outputs": [],
      "source": [
        "record_video('LunarLander-v2', model, video_length=1000, prefix='ppo-sem-treino')\n",
        "show_videos('videos', prefix='ppo-sem-treino')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A00W6yY3NkHG"
      },
      "source": [
        "## 3 - Visualização de resultados no Tensorboard"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Execute o comando abaixo para abrir o **Tensorboard**. Depois, execute o treinamento e acompanhe aqui os resultados, em tempo real."
      ],
      "metadata": {
        "id": "A0ZVznfSt_AL"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2MkjYOgx3itp"
      },
      "outputs": [],
      "source": [
        "%tensorboard --logdir log_dir"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r5UoXTZPNdFE"
      },
      "source": [
        "## 4 - Treina e salva o agente"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qT-lCHMOAHR3"
      },
      "source": [
        "Abaixo, rodamos a função de treinamento."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "e4cfSXIB-pTF"
      },
      "outputs": [],
      "source": [
        "# Treina por 200 mil passos\n",
        "model.learn(total_timesteps=200000)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MF5htH8TAK8m"
      },
      "source": [
        "Depois, gravamos o vídeo do agente treinado."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "olb37dOk_rgz"
      },
      "outputs": [],
      "source": [
        "record_video('LunarLander-v2', model, video_length=1000, prefix='ppo-treinado')\n",
        "show_videos('videos', prefix='ppo-treinado')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FN7XVTupAN-g"
      },
      "source": [
        "Por fim, salvamos o agente treinado em arquivo."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ygl_gVmV_QP7"
      },
      "outputs": [],
      "source": [
        "model.save(\"ppo_lunar\")\n",
        "del model  # delete trained model to demonstrate loading\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SwbRZv1yiUO3"
      },
      "source": [
        "## 5 - Carrega e avalia o agente"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "INlUVon9iflA"
      },
      "outputs": [],
      "source": [
        "model = PPO.load(\"ppo_lunar\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5SoCtCVAiX0O",
        "outputId": "cc46c3e4-537e-41c6-cae5-f5ddfe19813a"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/stable_baselines3/common/evaluation.py:69: UserWarning: Evaluation environment is not wrapped with a ``Monitor`` wrapper. This may result in reporting modified episode lengths and rewards, if other wrappers happen to modify these. Consider wrapping environment first with ``Monitor`` wrapper.\n",
            "  UserWarning,\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Retorno médio: 27.49 +/- 82.20\n"
          ]
        }
      ],
      "source": [
        "# Evaluate the trained agent\n",
        "mean_reward, std_reward = evaluate_policy(model, env, n_eval_episodes=30)\n",
        "\n",
        "print(f\"Retorno médio: {mean_reward:.2f} +/- {std_reward:.2f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aAdGk2ZFk0a2"
      },
      "source": [
        "Para treinar mais alguns passos, use `.set_env()` para definir o ambiente e chame novamente `.learn()`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cfv5y7qmkvRT"
      },
      "outputs": [],
      "source": [
        "model.set_env(gym.make(\"LunarLander-v2\"))\n",
        "model.learn(total_timesteps=100000)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [
        "FtY8FhliLsGm",
        "xVm9QPNVwKXN"
      ],
      "name": "cap09-PPO-stablebaselines.ipynb",
      "provenance": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3.9.13 ('rlx')",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.9.13"
    },
    "vscode": {
      "interpreter": {
        "hash": "27dbc9ce4cc602e4f15257b7b0018d8dff5b9ce9a7d73bc4399cb5afb1e02c4a"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}